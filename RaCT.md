# RACT: TOWARDS AMORTIZED RANKING-CRITICAL TRAINING FOR COLLABORATIVE FILTERING

## Ссылка на статью
https://arxiv.org/pdf/1906.04281.pdf

## Ссылка на код
https://github.com/samlobel/RaCT_CF

## Вольный перевод Abstract
Авторы предлагают новые методы для коллаборативной фильтрации, основанные на actor-critic RL. Это делается, чтобы напрямую максимизировать качество ранжирование. Они тренируют critic модель, которая будет приближать метрики ранжирования. А затем улучшают actor модель, чтобы напрямую улучшить ранжирующие метрики. По сравнению с обычными L2R (learning-to-rank) подходами, которые требуют повторного запуска для новых данных, предложенный метод позволяет получать приблизительные скоры для новых данных. Авторы показывают, что их метод значимо улучшает качество предсказаний множества моделей и достигает лучшего/сравнимого качества с сильными бейзлайнами.

## Краткий пересказ
С одной стороны, VAE показывают хорошее качество в задачах рекомендаций. С другой стороны, VAE максимизируют лишь правдоподобие. Авторы приводят пример с двумя векторами предсказаний (скорами). Один вариант имеет меньший лосс с точки зрения модели. Однако второй вектор лучше с точки зрения метрики ранжирования (NDCG). Получается, меньший лосс не гарантирует лучших рекомендаций. Проблема в том, что напрямую метрики ранжирования нельзя оптимизировать, так как они не дифференциируемы.

Для решения этой проблемы авторы приходят к идеи actor-critic из RL. Critic - это feature-based нейронка, которая приближает конечную метрику ранжирования. Actor - рекомендательная модель, например, VAE. Особенность critic-модели в том, что авторы создают признаки вручную, а не передают, например, выход из actor напрямую.
Про обучение actor (VAE) можно подробнее прочитать в соответствующих статьях. Авторы упоминают, что число параметров не зависит от количества пользователей - преимущество моделей этого вида.

RaCT выучивает диффиренциируемое представление ранжирующей метрики, которая представляется черным ящиком. Actor делает предсказание (action). Critic оценивает это действие какой-то оценкой на NDCG. Задача actor - максимизировать оценки critic.

В качестве critic  можно было бы взять конкатенацию предсказаний и ground-truth вектора, однако авторы пишут, что так не работает. Поэтому они предлагают сделать признаковой такую модельку. В качестве признаков берутся 3 величины: 1) лосс от actor модели, 2) количество айтемов в тесте у юзера, 3) количество айтемов в трейне у юзера. В качестве лосса для предсказания NDCG берется MSE.

Порядок обучения. Учим actor до сходимости. Учим critic до сходимости. Дальшем берем батч юзеров. И с помощью actor максимизируем предсказанный скор с помощью критика. (Здесь, правда, возникает вопрос, как посчитать градиент по второй и третьей статистики. Кажется, что градиент можно посчитать только по лоссу от actor. Если это так, то с помощью этого мы и можем пытаться максимизировать предсказания critic модели. Однако в статье я явного упоминания об этом не нашел). Когда меняются предсказания actor, нужно также обновлять предсказания critic

## Эксперименты

Интересно, что по картинкам добавление critic дает неплохой прирост. Однако по сводной таблице, такое нагромождение в виде идей из RL проигрывает простой модельки EASE
![Картинка 1](https://github.com/fotol1/RecSysPapersReviews/blob/master/images/ract_1.png)
![Картинка 2](https://github.com/fotol1/RecSysPapersReviews/blob/master/images/ract_2.png)

## Что я думаю по этому поводу?

Статья меня заинтересовала тем, что, кажется, critic можно тренировать не только на оффлайн метриках, но и в онлайн экспериментах. Например, critic по каким-то признакам из предсказаний основной модели должен научиться предсказывать какие-нибудь конверсии/клики/лайки и  любые другие онлайн метрики. А потом на основе этого, actor моделька должна будет пытаться максимизировать предсказания critic с зафиксированными параметрами.
